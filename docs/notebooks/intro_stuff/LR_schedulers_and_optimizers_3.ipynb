{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../images/logo-poster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Schedulers and Optimizers - focus on 32 batch size and AdamW\n",
    "\n",
    "We will revisit the first lab on the California Housing Dataset that we covered in this course in light of the theory on LR Schedulers and Optimizers that we have learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T04:52:43.740000Z",
     "start_time": "2020-09-17T04:52:43.735673Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam, SGD, AdamW, RMSprop, Adagrad\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from svlearn.california_housing.pre_process import preprocess_data\n",
    "from svlearn.california_housing.ingest_data import ingest_cal_housing_data\n",
    "from svlearn.approximator.regression_network import SimpleFeedForwardNet, \\\n",
    "                                                    SimpleNumpyDataset, \\\n",
    "                                                    create_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ingest_cal_housing_data()\n",
    "preprocessed_data = preprocess_data(data)\n",
    "x = preprocessed_data.drop(['y_target'], axis=1).to_numpy(dtype=np.float32)\n",
    "y = preprocessed_data[['y_target']].to_numpy(dtype=np.float32)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "dataset = SimpleNumpyDataset(x_train, y_train)\n",
    "val_dataset = SimpleNumpyDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some hyper parameters and the possible batch_sizes, optimizers, schedulers to iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = x_train.shape[1]\n",
    "N = x_train.shape[0]\n",
    "batch_sizes = [32]\n",
    "optimizers = [\"adamw\"]\n",
    "schedulers = [\"steplr\", \"exponentiallr\",\"reducelronplateau\", \"cosineannealinglr\"]\n",
    "epochs = 50\n",
    "max_steps_per_epoch = 1000\n",
    "drop_out = 0.1\n",
    "loss_function = MSELoss()\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to calculate validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)  # Accumulate the loss\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)  # Compute average loss over the full dataset\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dictionary of losses, learning rates, steps that will be used to plot the various graphs for each combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dict = {}\n",
    "steps_dict = {}\n",
    "lrs_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train double for loop that uses the optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(loader, val_loader, network, optimizer, scheduler):\n",
    "    losses = []\n",
    "    lrs = []\n",
    "    steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        steps_in_epoch = 0\n",
    "        for data, labels in loader:\n",
    "            optimizer.zero_grad()  # reset the parameter gradients\n",
    "            results = network(data, drop_out)  # get predictions\n",
    "            loss = loss_function(results, labels)  # estimate loss\n",
    "            loss.backward()  # back-propagate gradients\n",
    "            optimizer.step()  # update the parameter values (gradient-descent)\n",
    "            losses.append(loss.data)  # keep track of the loss of this epoch\n",
    "            steps +=1\n",
    "            steps_in_epoch +=1\n",
    "            if (steps_in_epoch > max_steps_per_epoch):\n",
    "                break\n",
    "                \n",
    "        val_loss = validate(network, val_loader)\n",
    "                \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        if scheduler:            \n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "        else:\n",
    "            lrs.append(lr)\n",
    "            \n",
    "    return losses,lrs,steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pseudocode\n",
    "For each of the choices of batch size   \n",
    "    For each of the choices of optimizers \n",
    "        For each of the choices of schedulers ( Step, Exponential, Plateau on validation loss, Cosine annealing)  \n",
    "\n",
    "            Compute the losses as a function of steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch_size in batch_sizes:   \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)    \n",
    "    for optim in optimizers:\n",
    "        for schedule in schedulers:\n",
    "            print(f\"Training with Batch size: {batch_size}, Optimizer: {optim}, Scheduler: {schedule}...\")\n",
    "            \n",
    "            # Initialize network for every iteration (so we restart with random weights and biases)\n",
    "            network = SimpleFeedForwardNet(input_dimension=dim_x, output_dimension=1)\n",
    "            \n",
    "            # Intialize optimizer\n",
    "            if optim == \"sgd_without_momentum\":\n",
    "                optimizer = SGD(network.parameters(), lr=lr)\n",
    "            elif optim == \"sgd\":\n",
    "                optimizer = SGD(network.parameters(), lr=lr, momentum=0.9)\n",
    "            elif optim == \"adam\":\n",
    "                optimizer = Adam(network.parameters(), lr=lr)\n",
    "            elif optim == \"adamw\":\n",
    "                optimizer = AdamW(network.parameters(), lr=lr)\n",
    "            elif optim == \"rmsprop\":\n",
    "                optimizer = RMSprop(network.parameters(), lr=lr)\n",
    "            else:\n",
    "                optimizer = Adagrad(network.parameters(), lr=lr)    \n",
    "                \n",
    "            # Initialize scheduler \n",
    "            scheduler = None\n",
    "            if schedule == \"steplr\":\n",
    "                scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "            elif schedule == \"exponentiallr\":\n",
    "                scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "            elif schedule == \"reducelronplateau\":\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "            elif schedule == \"cosineannealinglr\":\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=25)\n",
    "\n",
    "            # Call the train_loop method\n",
    "            losses, lrs, steps = train_loop(loader, val_loader, network, optimizer, scheduler)\n",
    "                    \n",
    "            losses_dict_key = f\"{batch_size}_{optim}_{schedule}\"\n",
    "            losses_dict[losses_dict_key] = [loss for loss in losses]\n",
    "            steps_dict[losses_dict_key] = steps\n",
    "            lrs_dict[losses_dict_key] = [lr for lr in lrs]\n",
    "            print(\"...finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_plots = len(losses_dict)\n",
    "\n",
    "# Create subplots with one row per plot\n",
    "fig, axes = plt.subplots(num_plots, 1, figsize=(8, 2*num_plots), sharex=True)\n",
    "\n",
    "# Generate a colormap with enough colors for each list\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "\n",
    "# Plot each list in its own subplot\n",
    "for i, (label, y_values) in enumerate(losses_dict.items()):\n",
    "    axes[i].plot(y_values, label=label, color=colors[i])\n",
    "    axes[i].set_ylim(0, 3)  # Set y-axis limits\n",
    "    axes[i].set_ylabel(\"Loss\")\n",
    "    axes[i].legend(loc=\"upper right\")\n",
    "\n",
    "# Set the x-axis label for the last subplot\n",
    "axes[-1].set_xlabel(\"Iterations\")\n",
    "\n",
    "# Add a title to the figure\n",
    "fig.suptitle(\"Schedulers Loss plots\", fontsize=16)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
