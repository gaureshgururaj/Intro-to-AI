{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../images/logo-poster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_function(activation_fn, x_range=(-10, 10), num_points=1000, title=None):\n",
    "    \"\"\"Function for plotting the passed in activation function\n",
    "\n",
    "    Args:\n",
    "        activation_fn (_type_): A valid pytorch activation function\n",
    "        x_range (tuple, optional): the range specified from a min value to a max value. Defaults to (-10, 10).\n",
    "        num_points (int, optional): The number of x-points on which to compute the y-value. Defaults to 1000.\n",
    "        title (_type_, optional): The title for the plot. Defaults to None, in which case it picks the name from the activation function passed.\n",
    "    \"\"\"\n",
    "    x = torch.linspace(x_range[0], x_range[1], num_points)\n",
    "    y = activation_fn(x)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x.numpy(), y.numpy(), label=title or activation_fn.__name__)\n",
    "    plt.title(title or activation_fn.__name__)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of some of the pytorch activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example activation functions\n",
    "activation_functions = [\n",
    "    F.relu,\n",
    "    F.sigmoid,\n",
    "    F.tanh,\n",
    "    F.elu,\n",
    "    F.leaky_relu,\n",
    "    F.softmax,  # Needs special treatment because of additional arguments that it takes\n",
    "    F.threshold, # Also needs special treatment for same reason\n",
    "    F.hardtanh, # Also needs special treatment for same reason\n",
    "]\n",
    "\n",
    "for activation_fn in activation_functions:\n",
    "    if activation_fn.__name__ == \"softmax\":\n",
    "        # Softmax needs to be handled differently because it requires a dimension\n",
    "        plot_activation_function(lambda x: activation_fn(x, dim=0), title=\"softmax\")\n",
    "    elif activation_fn.__name__ == \"_threshold\":\n",
    "        plot_activation_function(lambda x: activation_fn(x, threshold=4.0, value=4.0), title=\"threshold\")\n",
    "    elif activation_fn.__name__ == \"hardtanh\":\n",
    "        plot_activation_function(lambda x: activation_fn(x, min_val=-3, max_val=6.0), title=\"hardtanh\")\n",
    "    else:\n",
    "        plot_activation_function(activation_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
