{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb\n",
    "\n",
    "# path to save training result plots. Set this variable before running the notebook.\n",
    "dir_path = \"data/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic with PyTorch\n",
    "\n",
    "\n",
    "This is our first exercise with PyTorch. It comprises of writing a few simple classifiers and regressors. In particular, these models perform some arithmetic operations such determining the sum, standard-deviation, etc. for the sum, and determining if there is a negative number present in an array, or if the sum exceeds 100. A slightly more challenging task was to predict if any array contained a prime number.\n",
    "\n",
    "**Caveat Emptor**\n",
    "\n",
    "Remember, the purpose is **not to directly implement it as a PyTorch function, using the `torch` api directly** -- that\n",
    "would be too simple. Instead, the goal is to train a neural network to learn the underlying functions that can do this.\n",
    "\n",
    "Since neural networks are universal approximators, we therefore have to cast it into a form such that:\n",
    "$$ \\hat{y} = f(x) $$\n",
    "where $f(\\cdot)$ does the arithmetic operation (or an approximation of it).\n",
    "\n",
    "    \n",
    "## Homework assignment details\n",
    "\n",
    "The specific details of the homework assignment are here:\n",
    "\n",
    "Let us build a few classifier and regressor models from scratch. In particular, let us build the following models:\n",
    "\n",
    "### Classifiers\n",
    "* A model to detect if there is a negative number present in a set of 10 numbers as input. Output the result as a probability, and also a label, such as \"True\", \"False\"\n",
    "* A model to predict if the sum of the input numbers (10 of these) adds up to greater than 100. Once again, output a probability, and a label.\n",
    "* (Stretch goal) A model to detect the presence of a prime number in a set of 10 numbers given. Assume the numbers are less than a million.\n",
    "\n",
    "### Regressors\n",
    "* A model to find the sum of inputs (10 numbers as input)\n",
    "* A model to find the standard deviation of the inputs (10 numbers as input)\n",
    "* A model to find the maximum in a set of 10 numbers as input\n",
    "\n",
    "In order to train these models, you will have to do a careful generation of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from rich import print as rprint\n",
    "\n",
    "from svlearn.data.numpy_dataset_generator import NumpyDatasetGenerator\n",
    "from svlearn.nn.arithmetic_negative_number_detector import NegativeNumberDetector\n",
    "from svlearn.data.simple_torch_dataset import SimpleNumpyDataset\n",
    "from svlearn.train.classification_trainer import ClassificationTrainer\n",
    "from svlearn.train.regression_trainer import RegressionTrainer\n",
    "from svlearn.train.visualization_utils import visualize_classification_training_results , visualize_regression_training_results\n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss , MSELoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from svlearn.train.fit import ModelFitter\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summation of the input numbers\n",
    "\n",
    "Let us perform the following tasks\n",
    "\n",
    " 1. Generate the data\n",
    " 2. Instantiate the regressor\n",
    " 3. Train and evaluate the regressor with the generated data\n",
    " 4. Visualize the training results\n",
    "\n",
    "\n",
    "### Create a PyTorch dataset\n",
    "First let's create datasets for training and evaluation. `NumpyDatasetGenerator` generates all the datasets for this exercise. It creates both the samples `X` and the labels `y`associated with each sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "# create a numpy dataset for training\n",
    "train_dataset_generator = NumpyDatasetGenerator(num_samples=1000 , num_features=num_features)\n",
    "X , y = train_dataset_generator.generate_sum_features_dataset()\n",
    "\n",
    "# create a pytorch dataset from the numpy arrays\n",
    "train_dataset = SimpleNumpyDataset(X , y)\n",
    "\n",
    "# create a numpy dataset for evaluation\n",
    "eval_dataset_generator = NumpyDatasetGenerator(num_samples=300 , num_features=num_features)\n",
    "X , y = eval_dataset_generator.generate_sum_features_dataset()\n",
    "\n",
    "# create a pytorch dataset from the numpy arrays\n",
    "eval_dataset = SimpleNumpyDataset(X , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the 10 numbers as a pytorch array along with the label associated with the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Regressor\n",
    "\n",
    "Next, let's decide on how our neural network should be designed so that when provided with a sample of 10 numbers, it returns their sum. How should we go about this problem? \n",
    "\n",
    "Let's determine the inputs and outputs of model.\n",
    "\n",
    "the model needs to map 10 values to 1 , i.e. it's sum. \n",
    "So the input dimension is 10 and the output dimension is 1. \n",
    "\n",
    "Next, what transformation should we apply ? \n",
    "\n",
    "$$ x_1 + x_2 + x_3 + x_4 + x_5 + x_6 + x_7 + x_8 + x_9 + x_{10} = s $$\n",
    "\n",
    "We can rewrite the above as a linear transformation applied on a vector x as shown below\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "x_5 \\\\\n",
    "x_6 \\\\\n",
    "x_7 \\\\\n",
    "x_8 \\\\\n",
    "x_9 \\\\\n",
    "x_{10}\n",
    "\\end{bmatrix}\n",
    " + 0\n",
    "= s $$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\cdot \\mathbf{x} + b = s\n",
    "$$\n",
    "\n",
    "From this, we can conclude that a single linear transformation is sufficient to solve this problem. Note that no non-linear activation function is required at all! Now let's review the neural architecture of the `SumRegressor`\n",
    "\n",
    "### Instantiate model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.nn.arithmetic_sum_regressor import SumRegressor\n",
    "\n",
    "model = SumRegressor(input_dim=10)\n",
    "\n",
    "# print initial weights\n",
    "print(\"Model's state dict before training:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    rprint(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Trainer instance\n",
    "trainer = RegressionTrainer(\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=eval_dataset,\n",
    "    model=model,\n",
    "    loss_func=MSELoss(),\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=100,\n",
    "    batch_size=32,\n",
    "    device='cuda',\n",
    "    show_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.metrics_dict \n",
    "visualize_regression_training_results(metrics['train_loss'] , \n",
    "                                          metrics['eval_loss'] , \n",
    "                                          dir_path=dir_path , \n",
    "                                          filename=\"sum_regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the loss converges to 0 very rapidly. Let's view the weights of the model after the training. Have the weights converged to what we assumed it would?\n",
    "\n",
    "Note that we built the neural network with a single neuron -- we have only a linear operation to perform, namely the summation. Therefore, we use a single neuron, and eschew the activation function altogether, since we have no need for a deformation or non-linearity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print initial weights\n",
    "print(\"Model's state_dict after training:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    rprint(param_tensor, \"\\t\", model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect negative numbers\n",
    "\n",
    "What about detecting the negative numbers in an array? Can we do that with a single neuron? Do we need an activation function ?\n",
    "\n",
    "\n",
    "\n",
    "### Create dataset\n",
    "Let's first create a dataset by randomly sampling a normal distribution making sure that a signification portion of the distribution includes negative values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "# create a numpy dataset for training\n",
    "train_dataset_generator = NumpyDatasetGenerator(num_samples=1000 , num_features=num_features)\n",
    "X , y = train_dataset_generator.generate_negative_number_dataset(mean=50 , std=25)\n",
    "\n",
    "# create a pytorch dataset from the numpy arrays\n",
    "train_dataset = SimpleNumpyDataset(X , y)\n",
    "\n",
    "# create a numpy dataset for evaluation\n",
    "eval_dataset_generator = NumpyDatasetGenerator(num_samples=300 , num_features=num_features)\n",
    "X , y = eval_dataset_generator.generate_negative_number_dataset(mean=50 , std=25)\n",
    "\n",
    "# create a pytorch dataset from the numpy arrays\n",
    "eval_dataset = SimpleNumpyDataset(X , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model and trainer\n",
    "\n",
    "Let us think about the architecture for this network, and in particular, think about the activation function `ReLu` (Rectified Linear Unit). \n",
    "\n",
    "Recall that it turns off for all negative numbers, but leaves the positive values unaltered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NegativeNumberDetector(input_dim=num_features)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainer instance\n",
    "trainer = ClassificationTrainer(\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset, \n",
    "    model=model,\n",
    "    loss_func=BCELoss(),\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=200,\n",
    "    batch_size=32,\n",
    "    device='cuda',\n",
    "    show_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.metrics \n",
    "visualize_classification_training_results(metrics['train_loss'] , \n",
    "                                          metrics['eval_loss'] , \n",
    "                                          metrics['train_acc'] , \n",
    "                                          metrics['eval_acc'], \n",
    "                                          dir_path=dir_path, \n",
    "                                          filename=\"negative_numbers_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this problem we had to introduce non-linearity in addition to a linear transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect sum greater than 100\n",
    "\n",
    "This is essentially the same as the first  problem, recast as a binary classification problem.\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "train_dataset_generator = NumpyDatasetGenerator(num_samples=1000 , num_features=num_features)\n",
    "X , y = train_dataset_generator.generate_sum_greater_than_100_dataset()\n",
    "train_dataset = SimpleNumpyDataset(X , y)\n",
    "\n",
    "eval_dataset_generator = NumpyDatasetGenerator(num_samples=300 , num_features=num_features)\n",
    "X , y = eval_dataset_generator.generate_sum_greater_than_100_dataset()\n",
    "eval_dataset = SimpleNumpyDataset(X , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.nn.arithmetic_sum_greater_than_detector import SumGreaterThan100Detector\n",
    "\n",
    "# Assume train_dataset and eval_dataset are instances of NumpyDataset\n",
    "input_dim = 10\n",
    "model = SumGreaterThan100Detector(input_dim=input_dim)\n",
    "\n",
    "# model.apply(initialize_weights)\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainer instance\n",
    "trainer = ClassificationTrainer(\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=eval_dataset,\n",
    "    model=model,\n",
    "    loss_func=BCELoss(),\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=1000,\n",
    "    batch_size=64,\n",
    "    device='cuda',\n",
    "    show_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.metrics \n",
    "visualize_classification_training_results(metrics['train_loss'] , \n",
    "                                          metrics['eval_loss'] , \n",
    "                                          metrics['train_acc'] , \n",
    "                                          metrics['eval_acc'], \n",
    "                                          dir_path=dir_path , \n",
    "                                          filename=\"sum_greater_than_100_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation Regressor\n",
    "\n",
    " Variance (square of std) has quadratic terms, and thus is not linear in inputs. Therefore, you will need deformations to get higher order terms...and you will need more than 1 neuron for the same reason (to get both the quadratic and linear terms separately). You can convince yourself that a simple, small 2 layer-network can solve this, and empirically verify.\n",
    "\n",
    " We are instead going to deliberately take a more complex network to show that it converges much faster, so long as you have enough training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "train_dataset_generator = NumpyDatasetGenerator(num_samples=10000 , num_features=num_features)\n",
    "X , y = train_dataset_generator.generate_variable_std_dataset(mean=5)\n",
    "train_dataset = SimpleNumpyDataset(X , y)\n",
    "\n",
    "eval_dataset_generator = NumpyDatasetGenerator(num_samples=3000 , num_features=num_features)\n",
    "X , y = eval_dataset_generator.generate_variable_std_dataset(mean=5)\n",
    "eval_dataset = SimpleNumpyDataset(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.nn.arithmetic_std_regressor import StdRegressor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "# Assume train_dataset and eval_dataset are instances of NumpyDataset\n",
    "input_dim = 10\n",
    "model = StdRegressor(input_dim=input_dim)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Trainer instance\n",
    "trainer = RegressionTrainer(\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=eval_dataset,\n",
    "    model=model,\n",
    "    loss_func=MSELoss(),\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=300,\n",
    "    batch_size=1024,\n",
    "    device='cuda',\n",
    "    scheduler=scheduler,\n",
    "    show_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.metrics_dict \n",
    "visualize_regression_training_results(metrics['train_loss'] , \n",
    "                                          metrics['eval_loss'] , \n",
    "                                          dir_path=dir_path , \n",
    "                                          filename=\"std_regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max classifier\n",
    "\n",
    "Finally, for this exercise we do not need a neural network at all! Simply applying the softmax function will convert the list of numbers into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "train_dataset_generator = NumpyDatasetGenerator(num_samples=1000 , num_features=num_features)\n",
    "X , y = train_dataset_generator.generate_max_index_labels_dataset()\n",
    "train_dataset = SimpleNumpyDataset(X , y)\n",
    "\n",
    "sample_input , sample_target = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax\n",
    "\n",
    "torch.argmax(Softmax()(sample_input)) # prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime number detector\n",
    "\n",
    "This is a much harder problem; but let us reason through it. There are 78,498 primes below 1 million. Therefore, if we still only to an array or positive numbers, the probability that a number is prime and below a million would be: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 78_498/1_000_000\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability none of the 10 numbers being prime therefore is:\n",
    "\n",
    "no_primes = (1-p)**10\n",
    "no_primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, therefore, forms the lower bound on the accuracy of any classifier that detects primes. If the detector just blindly says that there is at-least one prime in the array, it would be right about 56% of the times.So a good classifier must beat a performance level of **56%** of accuracy for prime detection.\n",
    "\n",
    "But here we meet the inherent limitation of neural networks -- since there is no known continuous function that can generate the primes, therefore one cannot assume that (because of the Universal function approximation theorem,) a neural network will necessarily succeed in modeling primes.\n",
    "\n",
    "Indeed, the instructor gave this problem specifically to illustrate the current limits of what a neural network can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorial approximator?\n",
    "\n",
    "Let us ask this question -- can we have a neural network that can compute factorials? One would wonder -- how will we get training data for it, since factorials blow up exponentially, and quickly exceed the capacity of `float32` ? And we can train a network with a handful of few integers whose factorials are feasible.\n",
    "\n",
    "Fortunately, there is a generalization of factorial to the real-valued field: it is the $\\Gamma (x)$ function; therefore, we can generate an enormous amount to labeled training data:\n",
    "\n",
    "The Gamma function, $\\Gamma (x)$, is defined for all complex numbers except the non-positive integers. For any positive number $x$, it is defined as an integral from $0$ to $\\infty$:\n",
    "\n",
    "$$ \\Gamma(x) = \\int_{0}^{\\infty} t^{x-1} e^{-t} \\, dt $$\n",
    "\n",
    "This integral converges for $x > 0$. The Gamma function extends the concept of factorial (normally defined only for non-negative integers) to real and complex numbers, except for non-positive integers where the function is not defined. For positive integers $n$, it satisfies the relationship:\n",
    "\n",
    "$$ \\Gamma(n) = (n-1)! $$\n",
    "\n",
    "This means, for example, $\\Gamma(5) = 4!\\ = 24$, which extends the factorial notion to non-integer values smoothly. For non-integer values, the Gamma function can be computed using this integral definition or various numerical approximations and series expansions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "def generate_gamma_data(n: int = 100_000):\n",
    "\n",
    "    x = np.linspace(1, 22, n).reshape(-1, 1)\n",
    "    y = gamma(x).reshape(-1, 1)\n",
    "    return x-1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first verify that it works\n",
    "x, y = generate_gamma_data(22)\n",
    "for xx, yy in zip(x, y):\n",
    "    print(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we clearly see, it is working. We can verify that with $10!$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.factorial(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, let us train a neural network for this, and see what happens; read the associated neural network in the `svlearn.nn` module to see the simple feed-forward network we built for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.nn.multilayer_regressor import MultilayerRegressor\n",
    "from scipy.special import gamma\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "def generate_gamma_data(n: int = 100):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    x = 1+ np.linspace(0, 5, n)\n",
    "    np.random.shuffle(x)\n",
    " \n",
    "    y = gamma(x)\n",
    "    \n",
    "    y=y.reshape(-1, 1).astype(np.float32)\n",
    "    x = (x-1).reshape(-1, 1).astype(np.float32)\n",
    "    return x, y\n",
    "\n",
    "regressor = NeuralNetRegressor(\n",
    "    MultilayerRegressor,\n",
    "    max_epochs=50,\n",
    "    lr=0.001,\n",
    "    batch_size=1024,\n",
    "    optimizer=AdamW,\n",
    "    criterion=nn.MSELoss,\n",
    "    module__input_dimension=1,\n",
    "    module__output_dimension=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Create the training data\n",
    "from svlearn.arithmetic.data import ArithmeticData\n",
    "x_train, y_train = generate_gamma_data(100_000)\n",
    "x_test, y_test = generate_gamma_data(100_000)\n",
    "\n",
    "ModelFitter().regression_fit_evaluate_plot(regressor, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that with a smaller range, up to 5, in this case, the predictions are very good. However, as you increase the range of data, you will have to make quite some effort to think through your training data distribution, before you get better results, and you will have to train for much longer. This is primarily because of the exponential nature of the factorial. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
