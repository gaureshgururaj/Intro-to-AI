{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to compare the performance of a CNN architecture against the Fully Connected Neural Network architecture on the MNIST dataset. \n",
    "\n",
    "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "SOURCE: \n",
    "[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "The goal of the exercise is to classify the hand written digit images. \n",
    "We have used [the Inside Deep Learning book](https://www.manning.com/books/inside-deep-learning) - Chapter 3 as reference to create this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# custom functions\n",
    "from svlearn.train.simple_trainer import train_simple_network\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from svlearn.config.configuration import ConfigurationMixin\n",
    "from svlearn.common.utils import ensure_directory\n",
    "\n",
    "config = ConfigurationMixin().load_config()\n",
    "\n",
    "results_dir = config['mnist-classification']['results']\n",
    "ensure_directory(results_dir)\n",
    "data_dir = config['mnist-classification']['data']\n",
    "ensure_directory(data_dir)\n",
    "\n",
    "\n",
    "\n",
    "plot_style = config['plot_style']\n",
    "plt.rcParams.update(plot_style)\n",
    "plt.style.use('default')\n",
    "device=torch.device(config['device']) # <------------------ If you dont have \"cuda\" set it to \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Explore\n",
    "The MNIST dataset is available in PyTorch which we will download for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data_train =torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_data_test = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs , target = mnist_data_train[0]\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape of the input we can infer that the image has 1 channel , and the image size of (28 by 28 pixels).  Next, let's visualize this tensor as an image. We detach the tensor from the gradient tracking, then convert it to a numpy object after losing the extra channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_input = inputs.detach().cpu().squeeze().numpy()\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(detached_input, cmap='gray')\n",
    "plt.title(target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's print the tensor to see what the pixel values look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(detached_input) , np.max(detached_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values range between 0 and 1, which are already normalized. Let's proceed to defining the neural networks\n",
    "\n",
    "## Define Model Architecture\n",
    "\n",
    "We will now create two simple models: \n",
    "1. a fully connected neural network - with 2 linear layers\n",
    "2. a convolutional neural network - with a convolutional layer and a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28 # Width * Height\n",
    "\n",
    "channels = 1 # channels\n",
    "\n",
    "classes = 10 # number of target classes\n",
    "\n",
    "# metrics\n",
    "score_funcs = {'acc': accuracy_score }\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# try modifying these hyper-parameters to improve the model\n",
    "\n",
    "filters = 16 # number of convolution filters\n",
    "\n",
    "filter_dim = 3 # dimension of the filter (K x K)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "model_linear = nn.Sequential(\n",
    "  nn.Flatten(), # (Batch, Channel, Width, Height) is flattened to (Batch, Channel*Width*Height) = (Batch , input_dim) \n",
    "  nn.Linear(input_dim, 256), \n",
    "  nn.Tanh(), \n",
    "  nn.Linear(256, classes),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "model_cnn = nn.Sequential(\n",
    "  nn.Conv2d(channels, filters, filter_dim, padding=1),\n",
    "  nn.Tanh(),\n",
    "  nn.Flatten(), # (Batch, Channel, Width, Height) is flattened to (Batch, Channel*Width*Height) = (Batch , input_dim)  \n",
    "  nn.Linear(filters*input_dim, classes),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer `nn.Conv2d(channels, filters, filter_dim, padding=filter_dim//2),` uses a total of 16 filters of size 3 X 3. \n",
    "\n",
    "Therefore the input of (32 , 1, 28 , 28 ) is transformed to (32 , 16 , 28 , 28)\n",
    "Here we are padding the image, which adds extra pixels (with 0 value) along the border of the image to control the size of the image outputs of the convolution layer. Here we add a padding of size 1 to retain the image size in the output. \n",
    "\n",
    "To see how this works let's use this [online tool](https://ezyang.github.io/convolution-visualizer/) to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training what do the filters look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tensor = '0.weight' # weights of the first layer\n",
    "print(param_tensor)\n",
    "filters = model_cnn.state_dict()[param_tensor]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "for idx, filter in enumerate(filters):\n",
    "    plt.subplot(4, 4, idx+1)\n",
    "    plt.imshow(filter.detach().cpu().squeeze().numpy(), cmap='gray')\n",
    "    plt.title(idx, fontdict={\"size\": 8}, pad=1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create dataloaders for the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = DataLoader(mnist_data_train, batch_size=batch_size, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Convolutional Neural Network & Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linear = train_simple_network(model=model_linear,\n",
    "                           loss_func=nn.CrossEntropyLoss(),\n",
    "                           train_loader=mnist_train_loader,\n",
    "                           test_loader=mnist_test_loader,\n",
    "                           epochs=20,\n",
    "                           score_funcs=score_funcs,\n",
    "                           classify=True,\n",
    "                           checkpoint_file=f\"{results_dir}/fc-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn = train_simple_network(model=model_cnn,\n",
    "                           loss_func=nn.CrossEntropyLoss(),\n",
    "                           train_loader=mnist_train_loader,\n",
    "                           test_loader=mnist_test_loader,\n",
    "                           epochs=20,\n",
    "                           score_funcs=score_funcs,\n",
    "                           classify=True,\n",
    "                           checkpoint_file=f\"{results_dir}/cnn-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the performance\n",
    "\n",
    "Let's plot the validation accuracies to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams.update(plot_style)\n",
    "plt.style.use('ggplot')\n",
    "sns.lineplot(x='epoch', y='test acc', data=results_cnn, label='CNN')\n",
    "sns.lineplot(x='epoch', y='test acc', data=results_linear, label='Fully Connected');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple CNN model performs better than the Fully connected Neural Network just by swapping a linear layer by a convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the filters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_tensor = '0.weight' # weights of the first layer\n",
    "print(param_tensor)\n",
    "filters = model_cnn.state_dict()[param_tensor]\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "\n",
    "for idx, filter in enumerate(filters):\n",
    "    plt.subplot(4, 4, idx+1)\n",
    "    plt.imshow(filter.detach().cpu().squeeze().numpy(), cmap='gray')\n",
    "    plt.title(idx, fontdict={\"size\": 8}, pad=1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not very obvious what each filter is looking for in the image, so let's recreate the convolution operation from scratch using numpy to see what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a sample\n",
    "sample = mnist_data_test[1][0][0]\n",
    "sample = sample.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convolve(sample: np.array , filter: np.array) -> np.array:\n",
    "    \"\"\"applies the convolution filter to the sample\n",
    "\n",
    "    Args:\n",
    "        sample (np.array): the input image\n",
    "        filter (np.array): filter\n",
    "\n",
    "    Returns:\n",
    "        np.array: output image after convolution\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    H = sample.shape[0]\n",
    "    W = sample.shape[1]\n",
    "\n",
    "    filter_size = len(filter)\n",
    "\n",
    "    for i in range(H - (filter_size - 1)):\n",
    "        for j in range (W - (filter_size - 1)):\n",
    "\n",
    "            segment = padded_sample[i:i+filter_size, j:j+filter_size]\n",
    "            result = np.sum(segment * filter)\n",
    "            results.append(result)\n",
    "    \n",
    "    return np.array(results).reshape((28 , 28))\n",
    "\n",
    "# padding the sample \n",
    "padded_sample = np.pad(sample , pad_width=1)\n",
    "\n",
    "# choosing the first filter to test. Modify this to see the effect of different filters\n",
    "filter_id = 1\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# for each filter apply the convolution\n",
    "for idx, filter in enumerate(filters):\n",
    "    filter = filter.detach().cpu().squeeze().numpy()\n",
    "    convolved = convolve(padded_sample , filter)\n",
    "\n",
    "    plt.subplot(4, 4, idx+1)\n",
    "    plt.imshow(convolved, cmap='gray')\n",
    "    plt.title(idx, fontdict={\"size\": 8}, pad=1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each filter is capturing a different spacial feature of the image. Edges, horizontal lines, vertical lines and many combinations of other features are highlighted by these filters. And fascinatingly this ability to learn spacial patterns , help CNNs outperform fully connected networks with fewer weights! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
