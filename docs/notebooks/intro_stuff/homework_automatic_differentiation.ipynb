{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f85ea6",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with PyTorch\n",
    "\n",
    "## Objective\n",
    "As a continuation to the theory session, we will explore the powerful concept of automatic differentiation using PyTorch. We will start by creating a tensor and applying a series of transformations to obtain a scalar value. Through this process, we will explore how PyTorch tracks operations for gradient computation, utilize the backward method to compute gradients, and manually calculate gradients to validate your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d18fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant libraries\n",
    "import torch \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43639d",
   "metadata": {},
   "source": [
    "Before proceeding with this exercise read about Autograd from the pytorch documentation\n",
    "\n",
    "[Autograd](https://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321926b",
   "metadata": {},
   "source": [
    "## Create a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4991646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor 'x' with 21 ( 20 + 1 to include the end value) evenly spaced values from -5 to 5,\n",
    "# Remember to enable it's gradient tracking  \n",
    "\n",
    "x = None\n",
    "\n",
    "# Print the value of 'x'\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e851d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shape of x? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77be9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor y that is the square of x\n",
    "y = None\n",
    "\n",
    "# Print y\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30449857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor z that is adds 5 to y\n",
    "\n",
    "z = None\n",
    "\n",
    "# Print z\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90262caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that sums all values of z\n",
    "l = None\n",
    "\n",
    "# Print l\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot z against x. Note that we use .detach() which removes the returned tensor from the graph\n",
    "# plt.plot(x.detach(), z.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d703295",
   "metadata": {},
   "source": [
    "The grad function of a tensor links the tensor to the function that created it. But how is this useful? Read the documentation and provide your answer below.\n",
    "\n",
    "ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4078156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print y's grad function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3ea1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print z's grad function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432886ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print l's grad function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4407fb",
   "metadata": {},
   "source": [
    "The variables $x$, $y$, $z$ and $l$ are nodes in a directed acyclic graph. \n",
    "\n",
    "$l\\longrightarrow z\\longrightarrow y \\longrightarrow x$ . $l$ is the root node. $x$ is the leaf node.\n",
    "* $l$ is connected to $z$ by $l$'s grad function: SumBackward\n",
    "* $z$ is connected to $y$ by $z$'s grad function: AddBackward\n",
    "* $y$ is connected to $x$ by $y$'s grad function: PowBackward\n",
    "\n",
    "To learn more about the computational graphs that get created read : https://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html\n",
    "\n",
    "\n",
    "To view the gradient vector of a tensor use : `tensor.grad`. Note only the input vectors (leaf nodes) store gradients. If you wish to record the gradients of y and z you can add a .`retain_grad()` to the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55645204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the gradient vector of x currently?\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccff13a",
   "metadata": {},
   "source": [
    "Before computing the gradients using pytorch, can we derive the gradients using calculus ? \n",
    "\n",
    "We are interested in computing the gradients of each element of the $\\mathbf{x}$ tensor i.e.\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial x_i} = ? $$ \n",
    "\n",
    "Applying the chain rule (refer the theory session videos)\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial x_i} = \\frac{\\partial l}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x_i}$$ \n",
    "\n",
    "What is the gradient of $l$ with respect to $\\mathbf{z}$ ? Since $l$ is a scalar and $\\mathbf{z}$ is a vector, let's think about computing the  gradient w.r.t. to an element of $z$ namely, $z_i$. If $z_i$ changes by a small amount, $l$ (the sum) will also change by the same amount. Therefore,\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial z_i} = 1 $$  \n",
    "\n",
    "\n",
    "Next lets focus on the derivative of $z_i$ w.r.t to corresponding element in y, namely $y_i$. \n",
    "$$ \\frac{\\partial z_i}{\\partial y_i} = ? $$ \n",
    "\n",
    "then, \n",
    "$$ \\frac{\\partial y_i}{\\partial x_i} = ? $$\n",
    "\n",
    "Combining all together we get: \n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial x_i} = ? $$ \n",
    "\n",
    "Let's now use pytorch to back propagate and check if the gradients match with our derivation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5eb33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compute the gradient of l with respect to x, using .backward() method\n",
    "\n",
    "\n",
    "\n",
    "# display the gradients of x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660deef7",
   "metadata": {},
   "source": [
    "Verify that the gradients match the computed partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6fb7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grad_derived = None\n",
    "\n",
    "# print(x.grad == x_grad_derived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6accef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot x with x.grad, does it match your expectation?  \n",
    "\n",
    "# plt.plot(x.detach(), x.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903746d",
   "metadata": {},
   "source": [
    "As seen in this exercise Autograd simplifies the process of computing gradients, which are essential for updating model parameters during training. By automatically handling the complex chain rule of calculus, autograd allows for efficient and accurate gradient computation, enabling the optimization algorithms to adjust weights and biases effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
