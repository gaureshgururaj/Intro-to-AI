{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.functional import (threshold ,\n",
    "                                 relu ,relu_ , \n",
    "                                 hardtanh , hardtanh_, \n",
    "                                 relu6 , \n",
    "                                 gelu,\n",
    "                                 elu, elu_ , \n",
    "                                 selu , \n",
    "                                 celu,\n",
    "                                 leaky_relu , \n",
    "                                 leaky_relu_ , \n",
    "                                 prelu , \n",
    "                                 rrelu,\n",
    "                                 rrelu_ ,\n",
    "                                 glu ,\n",
    "                                 logsigmoid,\n",
    "                                 hardshrink ,\n",
    "                                 tanhshrink ,\n",
    "                                 softsign ,\n",
    "                                 softplus,\n",
    "                                 softmin ,\n",
    "                                 softmax ,\n",
    "                                 softshrink ,\n",
    "                                 gumbel_softmax,\n",
    "                                 log_softmax ,\n",
    "                                 hardsigmoid ,\n",
    "                                 silu ,\n",
    "                                 mish ,\n",
    "                                 tanh,\n",
    "                                 sigmoid,\n",
    "                                 batch_norm ,\n",
    "                                 group_norm ,\n",
    "                                 instance_norm,\n",
    "                                 layer_norm,\n",
    "                                 local_response_norm,\n",
    "                                 rms_norm,\n",
    "                                 normalize\n",
    "                                 )\n",
    "\n",
    "from torch.nn import Threshold , PReLU\n",
    "\n",
    "def plot(func: callable) -> None:\n",
    "    \"\"\"plots the given function between -10 and 10\n",
    "\n",
    "    Args:\n",
    "        func (callable): the function to plot\n",
    "    \"\"\"\n",
    "    x = torch.tensor(np.arange(-10 , 10 , 0.1))\n",
    "    y = func(x).numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.plot(x.numpy(), y)\n",
    "    fig.suptitle(func.__name__, fontsize=16)\n",
    "    plt.xlabel('x',  fontsize=12)\n",
    "    plt.ylabel(f\"{func.__name__}(x)\",  fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "## 1. Sigmoid\n",
    "\n",
    "**Formula:**\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Use Case:**\n",
    "The sigmoid function is widely used in binary classification problems, where the output is expected to be between 0 and 1. It's particularly useful in the output layer of a neural network for binary classification tasks like spam detection, where the output probability can directly represent the confidence of the prediction.\n",
    "\n",
    "**Advantages:**\n",
    "Sigmoid functions are smooth and differentiable, making them suitable for gradient-based optimization techniques. They also map input values to a bounded range [0, 1], which can be interpreted as probabilities, making them useful in probabilistic models and classification problems.\n",
    "\n",
    "**Disadvantages:**\n",
    "The sigmoid function suffers from vanishing gradients, particularly for very high or low input values. This can cause the weights of the earlier layers to update very slowly during backpropagation, leading to slow convergence and sometimes poor performance in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tanh\n",
    "\n",
    "**Formula:**\n",
    "$$ \\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "**Use Case:**\n",
    "Tanh activation functions are typically used in recurrent neural networks (RNNs) where the output needs to be centered around zero. It is also favored in scenarios where data is normalized to have zero mean, such as in NLP tasks, to ensure that the gradients are more balanced.\n",
    "\n",
    "**Advantages:**\n",
    "The tanh function outputs values between -1 and 1, which helps in centering the data, making the optimization process more efficient. It also has a steeper gradient compared to sigmoid, which reduces the problem of vanishing gradients to some extent.\n",
    "\n",
    "**Disadvantages:**\n",
    "Like sigmoid, tanh also suffers from the vanishing gradient problem, especially for extreme input values. This can lead to slow learning or even stagnation in deep networks, particularly in scenarios where the network has many layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "**Use Case:**\n",
    "ReLU is the default activation function in most modern neural networks, especially in convolutional neural networks (CNNs) used for image recognition. It is often used in the hidden layers of deep neural networks due to its simplicity and efficiency in computation.\n",
    "\n",
    "**Advantages:**\n",
    "ReLU is computationally efficient because it requires only a simple threshold operation. It also helps in mitigating the vanishing gradient problem since the gradient is either 0 or 1, allowing for faster and more effective learning in deep networks.\n",
    "\n",
    "**Disadvantages:**\n",
    "ReLU can suffer from the \"dying ReLU\" problem, where neurons can become inactive and output zero for all inputs if they are updated in such a way that they only output negative values, leading to a lack of learning in those neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leaky ReLU\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      \\alpha x & \\text{if } x \\leq 0 \n",
    "   \\end{cases} $$\n",
    "Where $\\alpha$ is a small constant, typically 0.01.\n",
    "\n",
    "**Use Case:**\n",
    "Leaky ReLU is often used in scenarios where the network might suffer from the \"dying ReLU\" problem. It is particularly useful in generative models like GANs and in networks where negative inputs carry meaningful information.\n",
    "\n",
    "**Advantages:**\n",
    "Leaky ReLU allows a small, non-zero gradient when the unit is not active, which helps prevent neurons from becoming inactive and improves the learning process by ensuring all neurons contribute to the gradient descent.\n",
    "\n",
    "**Disadvantages:**\n",
    "The introduction of a negative slope adds an additional hyperparameter that needs tuning. Additionally, the choice of the slope is often arbitrary and might require experimentation to find the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exponential Linear Unit (ELU)\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "   \\end{cases} $$\n",
    "Where $\\alpha$ is a positive constant.\n",
    "\n",
    "**Use Case:**\n",
    "ELU is commonly used in deep networks, particularly in tasks involving sequential data and time series forecasting. Its ability to produce negative outputs helps in centering the data, which can lead to faster learning.\n",
    "\n",
    "**Advantages:**\n",
    "ELU helps to mitigate the vanishing gradient problem and has the advantage of producing negative outputs, which allows for a mean activation closer to zero. This helps in reducing bias shifts, leading to faster convergence.\n",
    "\n",
    "**Disadvantages:**\n",
    "The main disadvantage of ELU is the increased computational cost due to the exponential operation in its formula. Additionally, the choice of the $\\alpha$ parameter can significantly affect the performance and may require fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaled Exponential Linear Unit (SELU)\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\lambda \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "   \\end{cases} $$\n",
    "Where $\\lambda$ and $\\alpha$ are constants.\n",
    "\n",
    "**Use Case:**\n",
    "SELU is used primarily in self-normalizing neural networks (SNNs), where maintaining a standardized mean and variance throughout the layers is crucial. It is particularly effective in deep neural networks where maintaining a stable distribution is necessary for efficient learning.\n",
    "\n",
    "**Advantages:**\n",
    "SELU is designed to ensure that the mean and variance of inputs remain standardized across layers. This self-normalizing property reduces the need for batch normalization and can lead to faster and more stable training, particularly in deep networks.\n",
    "\n",
    "**Disadvantages:**\n",
    "SELU requires careful initialization of weights and specific network architectures to function effectively. It can also be sensitive to hyperparameter choices, and its effectiveness may diminish in certain types of networks, particularly those not designed for self-normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(selu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Swish\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = x \\cdot \\sigma(x) $$\n",
    "Where $\\sigma(x)$ is the sigmoid function.\n",
    "\n",
    "**Use Case:**\n",
    "Swish is commonly used in deep learning tasks such as image classification and object detection, where the network's ability to learn complex patterns is critical. It is also favored in networks where smooth activation functions are beneficial.\n",
    "\n",
    "**Advantages:**\n",
    "Swish is a smooth and non-monotonic function that has been shown to outperform ReLU in some deep networks. Its ability to allow small negative values helps in preserving information during forward and backward passes, leading to improved gradient flow and potentially better learning.\n",
    "\n",
    "**Disadvantages:**\n",
    "Swish introduces a slight computational overhead compared to ReLU due to the need to calculate the sigmoid function. Additionally, its non-monotonicity can lead to complex optimization landscapes, which might complicate the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x: torch.Tensor):\n",
    "    return (x * sigmoid(x))\n",
    "\n",
    "plot(swish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mish\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = x \\cdot \\tanh(\\text{softplus}(x)) $$\n",
    "Where $\\text{softplus}(x) = \\log(1 + e^x)$.\n",
    "\n",
    "**Use Case:**\n",
    "Mish is often used in tasks that require a smooth and continuous activation function, such as image classification, NLP, and reinforcement learning. It has been particularly effective in deep networks where gradient flow and preserving information are critical.\n",
    "\n",
    "**Advantages:**\n",
    "Mish is a smooth, non-monotonic activation function that has shown promising results in various benchmarks, often outperforming ReLU and Swish. Its ability to maintain a strong gradient flow and avoid the dead neuron problem makes it a robust choice for deep networks.\n",
    "\n",
    "**Disadvantages:**\n",
    "The main disadvantage of Mish is its computational complexity compared to simpler functions like ReLU. The combination of tanh and softplus adds to the computational cost, which can be a concern in resource-constrained environments or very large networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Softplus\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\log(1 + e^x) $$\n",
    "\n",
    "**Use Case:**\n",
    "Softplus is used in tasks where a smooth approximation of ReLU is preferred, such as in some reinforcement learning algorithms and probabilistic models. It is also used in networks where avoiding sharp transitions in the activation function is crucial.\n",
    "\n",
    "**Advantages:**\n",
    "Softplus is a smooth approximation of ReLU and does not suffer from the \"dying ReLU\" problem. Its differentiable nature makes it useful in scenarios where smooth gradients are necessary for efficient learning.\n",
    "\n",
    "**Disadvantages:**\n",
    "Softplus can be computationally more expensive than ReLU due to the logarithmic and exponential operations. It also suffers from slower convergence in some cases, particularly in deep networks, where sharper transitions in the activation function might be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(softplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Softmax\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "\n",
    "**Use Case:**\n",
    "Softmax is predominantly used in the output layer of classification networks, particularly in multi-class classification tasks. It converts the logits from the network into probabilities that sum to one, making it suitable for problems like digit classification in the MNIST dataset.\n",
    "\n",
    "**Advantages:**\n",
    "Softmax provides a probabilistic interpretation of the network's output, which is essential in classification tasks. It also enables the network to learn from the relative differences between output classes, improving the overall accuracy in multi-class problems.\n",
    "\n",
    "**Disadvantages:**\n",
    "The main disadvantage of Softmax is that it can lead to vanishing gradients during backpropagation, especially when one class is significantly more likely than others. Additionally, it assumes that the classes are mutually exclusive, which may not be suitable for multi-label classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Softmin\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x_i) = \\frac{e^{-x_i}}{\\sum_{j=1}^{n} e^{-x_j}} $$\n",
    "\n",
    "**Use Case:**\n",
    "Softmin is used in scenarios where a probability distribution with a preference for smaller values is needed. It is particularly useful in optimization tasks where minimizing a particular cost function or objective is the goal. For example, in ranking problems or in some reinforcement learning tasks, where the goal is to select actions that minimize a cost, Softmin can be applied to the action-value function to obtain the optimal policy.\n",
    "\n",
    "**Advantages:**\n",
    "Softmin transforms logits into a probability distribution that emphasizes lower values, making it ideal for tasks where smaller outputs are preferred. This can be particularly useful in applications like risk assessment, where the aim is to select the least risky option. Additionally, Softmin ensures that the probabilities sum to one, providing a clear interpretation of the results, similar to the Softmax function but inverted in terms of preference.\n",
    "\n",
    "**Disadvantages:**\n",
    "The primary disadvantage of Softmin is its computational complexity, which is similar to Softmax. The exponentiation of negative values can lead to numerical instability, particularly when dealing with very large or small input values. Additionally, in tasks where larger values represent better outcomes, Softmin may not be suitable, as it inherently biases the output towards smaller values, which may not always be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(softmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Hard Sigmoid\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\begin{cases} \n",
    "      0 & \\text{if } x \\leq -2.5 \\\\\n",
    "      1 & \\text{if } x \\geq 2.5 \\\\\n",
    "      0.2x + 0.5 & \\text{otherwise} \n",
    "   \\end{cases} $$\n",
    "\n",
    "**Use Case:**\n",
    "Hard Sigmoid is used in mobile and embedded systems where computational efficiency is crucial. It is often employed in tasks requiring binary outputs but where the full computational complexity of the sigmoid function is not justified.\n",
    "\n",
    "**Advantages:**\n",
    "Hard Sigmoid offers a good trade-off between computational efficiency and the benefits of a sigmoid-like function. It is faster to compute and requires fewer resources, making it suitable for deployment in low-power or real-time systems.\n",
    "\n",
    "**Disadvantages:**\n",
    "The approximation in Hard Sigmoid can lead to a loss of precision and smoothness, which might affect the performance of the network in tasks where fine-grained decisions are critical. It also suffers from the same vanishing gradient problem as the standard sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hardsigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Hard Tanh\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\begin{cases} \n",
    "      -1 & \\text{if } x \\leq -1 \\\\\n",
    "      1 & \\text{if } x \\geq 1 \\\\\n",
    "      x & \\text{otherwise} \n",
    "   \\end{cases} $$\n",
    "\n",
    "**Use Case:**\n",
    "Hard Tanh is often used in resource-constrained environments where the computational cost of the standard tanh function is too high. It is also used in binary classification tasks where quick computation is essential, such as in real-time systems.\n",
    "\n",
    "**Advantages:**\n",
    "Hard Tanh is computationally efficient due to its piecewise linear nature. It retains some of the benefits of tanh, such as centering the data around zero, while being faster to compute, making it suitable for deployment in environments with limited computational resources.\n",
    "\n",
    "**Disadvantages:**\n",
    "The primary disadvantage of Hard Tanh is the loss of smoothness compared to the standard tanh function. This can lead to less precise gradients and potentially slower convergence during training, particularly in tasks where the smooth gradient flow is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hardtanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Hard Swish\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = x \\cdot \\text{Hard Sigmoid}(x) $$\n",
    "\n",
    "**Use Case:**\n",
    "Hard Swish is used in mobile and embedded networks where computational efficiency is paramount. It is particularly effective in tasks that benefit from the smoothness of Swish but require faster computation, such as image classification on mobile devices.\n",
    "\n",
    "**Advantages:**\n",
    "Hard Swish provides a good balance between the computational efficiency of hard sigmoid and the smoothness of the Swish function. It is faster to compute than Swish and retains some of its benefits, making it suitable for deployment in low-latency environments.\n",
    "\n",
    "**Disadvantages:**\n",
    "Hard Swish, being an approximation, can lead to reduced accuracy in tasks where the smoothness of the activation function is crucial. It also introduces additional complexity in the forward pass compared to simpler functions like ReLU, which might not be justified in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardswish(x):\n",
    "    return x * hardsigmoid(x)\n",
    "\n",
    "plot(hardswish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Log-Sigmoid\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\log\\left(\\frac{1}{1 + e^{-x}}\\right) $$\n",
    "\n",
    "**Use Case:**\n",
    "Log-Sigmoid is used in binary classification tasks where a logarithmic scale is preferred, particularly in probabilistic models where log-probabilities are used for stability. It is also used in scenarios where the output needs to be within a range that is smooth and continuously differentiable, such as in certain types of neural networks that model likelihoods or probabilities.\n",
    "\n",
    "**Advantages:**\n",
    "Log-Sigmoid provides a smooth activation function that maps input values to a bounded range, making it suitable for probabilistic interpretations. The logarithmic nature of the function helps in stabilizing gradients, particularly in networks where logarithms are used for loss functions, such as in log-likelihood estimation. This can lead to more stable training and better performance in certain models.\n",
    "\n",
    "**Disadvantages:**\n",
    "Log-Sigmoid suffers from similar issues as the standard sigmoid function, including the vanishing gradient problem. The logarithmic transformation can also make the function more computationally intensive, which might be a concern in resource-constrained environments. Additionally, it compresses the input space more than the standard sigmoid, which can lead to a loss of information, particularly in deep networks where the input range is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(logsigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Gaussian\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = e^{-x^2} $$\n",
    "\n",
    "**Use Case:**\n",
    "The Gaussian activation function is used in radial basis function networks and some types of autoencoders. It is particularly effective in tasks that require a localized response, such as in clustering and some types of anomaly detection.\n",
    "\n",
    "**Advantages:**\n",
    "The Gaussian activation function provides a localized response, making it useful in networks where the activation needs to be centered around a specific input value. This can be particularly beneficial in tasks like clustering, where the model needs to focus on specific regions of the input space.\n",
    "\n",
    "**Disadvantages:**\n",
    "The Gaussian activation function can suffer from vanishing gradients, particularly for inputs far from the center of the Gaussian. This can lead to slow learning or even stagnation in some cases, particularly in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x):\n",
    "    return torch.exp(-x**2)\n",
    "\n",
    "plot(gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Gaussian Error Linear Unit (GELU)\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = x \\cdot \\Phi(x) $$\n",
    "Where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "**Use Case:**\n",
    "GELU is commonly used in transformer models, particularly in natural language processing tasks like machine translation and text classification. It is also effective in image processing tasks where smooth and probabilistic activation functions are beneficial.\n",
    "\n",
    "**Advantages:**\n",
    "GELU provides a smooth, probabilistic activation function that has been shown to improve performance in a variety of deep learning tasks. Its ability to maintain a strong gradient flow and avoid the dead neuron problem makes it a robust choice for deep networks.\n",
    "\n",
    "**Disadvantages:**\n",
    "The main disadvantage of GELU is its computational complexity compared to simpler functions like ReLU. The need to compute the cumulative distribution function adds to the computational cost, which can be a concern in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Softsign\n",
    "\n",
    "**Formula:**\n",
    "$$ f(x) = \\frac{x}{1 + |x|} $$\n",
    "\n",
    "**Use Case:**\n",
    "Softsign is used in networks where a smooth approximation to the identity function is desired, similar to tanh but with a different scaling. It is particularly effective in tasks that require a smooth gradient flow, such as in some types of autoencoders and reinforcement learning.\n",
    "\n",
    "**Advantages:**\n",
    "Softsign provides a smooth, non-linear activation function that scales better than tanh in some cases, particularly for large input values. Its smooth gradient flow can help in tasks that require precise gradient-based optimization.\n",
    "\n",
    "**Disadvantages:**\n",
    "Softsign can suffer from vanishing gradients for large input values, similar to tanh. This can lead to slow learning or even stagnation in deep networks, particularly in tasks where strong gradients are necessary for effective learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(softsign)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
