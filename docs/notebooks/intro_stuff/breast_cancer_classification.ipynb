{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../images/logo-poster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for classification of the breast cancer dataset\n",
    "\n",
    "We will extend on the simple neural architecture to now solve a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.approximator.regression_network import SimpleFeedForwardNet, \\\n",
    "                                                    SimpleNumpyDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "We will use the breast cancer dataset for building this neural network classification model\n",
    "\n",
    "## Homework \n",
    "\n",
    "Study the preprocess_data to understand the transformations being done on the ingested raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from svlearn.breast_cancer.pre_process import preprocess_data\n",
    "from svlearn.breast_cancer.ingest_data import ingest_breast_cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ingest_breast_cancer_data()\n",
    "preprocessed_data = preprocess_data(data)\n",
    "x = preprocessed_data.drop(['target'], axis=1).to_numpy(dtype=np.float32)\n",
    "y = preprocessed_data[['target']].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = x.shape[1]\n",
    "X_tensor = torch.from_numpy(x).reshape(-1, dim_x)\n",
    "y_tensor = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training parts ready\n",
    "network = SimpleFeedForwardNet(input_dimension=dim_x, output_dimension=1)\n",
    "print(network)\n",
    "network.activation = torch.relu # Rectified Linear Unit\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "optimizer = Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "# Let us now train the network\n",
    "losses = []\n",
    "epochs = 2001\n",
    "drop_out = 0.1\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()  # reset the gradients\n",
    "    results = network(X_tensor, drop_out)  # get predictions\n",
    "    loss = loss_function(results, y_tensor)  # estimate loss\n",
    "    loss.backward()  # back-propagate gradients\n",
    "    optimizer.step()  # update the parameter values (gradient-descent)\n",
    "    losses.append(loss.data)  # keep track of the loss of this epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of batch gradient descent on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.approximator.regression_network import create_plots\n",
    "\n",
    "create_plots(epochs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "Here, at each step, we learn from from mini batches of data. So, for this, the dataloader returns data in small batches. The mini-batch size is specified in the pytorch dataloader by the parameter `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleNumpyDataset(x, y)\n",
    "loader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "\n",
    "epochs = 101\n",
    "drop_out = 0.1\n",
    "# Get the training parts ready\n",
    "network = SimpleFeedForwardNet(input_dimension=dim_x, output_dimension=1)\n",
    "print(network)\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "optimizer = Adam(network.parameters(), lr=0.001)\n",
    "losses = []\n",
    "steps = 0\n",
    "for epoch in range(epochs):\n",
    "    start = True\n",
    "    for data, labels in loader:\n",
    "        optimizer.zero_grad()  # reset the parameter gradients\n",
    "        results = network(data, drop_out)  # get predictions\n",
    "        loss = loss_function(results, labels)  # estimate loss\n",
    "        loss.backward()  # back-propagate gradients\n",
    "        optimizer.step()  # update the parameter values (gradient-descent)\n",
    "        losses.append(loss.data)  # keep track of the loss of this epoch\n",
    "        if epoch % 10 == 0 and start:\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.data))\n",
    "        start = False\n",
    "        steps +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of MINI-BATCH gradient descent on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(steps, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Here, each step of learning is from one datum. So an epoch will have as many steps as the training sample size. Let us see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we have set the mini-batch size to 1!\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "epochs = 50\n",
    "drop_out = 0.1\n",
    "# Get the training parts ready\n",
    "network = SimpleFeedForwardNet(input_dimension=dim_x, output_dimension=1)\n",
    "\n",
    "print(network)\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "optimizer = SGD(network.parameters(), lr=0.001)\n",
    "losses = []\n",
    "steps = 0\n",
    "for epoch in range(epochs):\n",
    "    start = True\n",
    "    for data, labels in loader:\n",
    "        optimizer.zero_grad()  # reset the parameter gradients\n",
    "        results = network(data, drop_out)  # get predictions\n",
    "        loss = loss_function(results, labels)  # estimate loss\n",
    "        loss.backward()  # back-propagate gradients\n",
    "        optimizer.step()  # update the parameter values (gradient-descent)\n",
    "        losses.append(loss.data)  # keep track of the loss of this epoch\n",
    "        if epoch % 5 == 0 and start:\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.data))\n",
    "        start = False\n",
    "        steps +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of Stochastic gradient descent optimization based learning from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(steps, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
