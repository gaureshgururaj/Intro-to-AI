{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-02T04:35:40.397498Z",
     "start_time": "2022-01-02T04:35:38.814341Z"
    },
    "cell_style": "center",
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "#  Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this notebook, we will do the process of learning an effective model through  gradient descent, by working out the steps on our own. \n",
    "\n",
    "For simplicity, we will consider the case of binary classification using a binary cross entropy loss.\n",
    "\n",
    "We will use a simple logistic regression model to classify between two species of the iris dataset using a single predictor variable sepal length\n",
    "\n",
    "Let us repeat the exercise, but write our own code for the gradient descent based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data\n",
    "\n",
    "Let us load the data into a Pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:57.148666Z",
     "start_time": "2021-10-14T20:08:56.793731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame with the features\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Scale the features using the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler to the features\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Add the target (species) to the DataFrame\n",
    "df['target'] = iris.target\n",
    "\n",
    "# If you want to have species names instead of numbers\n",
    "df['species'] = df['target'].map({i: species for i, species in enumerate(iris.target_names)})\n",
    "\n",
    "# Shuffle it\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the dataframe only to those other than `setosa` and only consider the two predictor variables `sepal length` and `petal length` for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_df = df[df['species'] != 'setosa']\n",
    "restricted_df['target'] = restricted_df['target'] - 1\n",
    "restricted_df = restricted_df[['sepal length (cm)', 'petal length (cm)', 'target']].rename(columns={'sepal length (cm)': 'x', 'petal length (cm)': 'y'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first visualize a scatter plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:58.499168Z",
     "start_time": "2021-10-14T20:08:57.730221Z"
    },
    "cell_style": "split",
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(restricted_df['x'], restricted_df['y'], c=restricted_df['target'], cmap='viridis', edgecolor='k')\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(scatter, label='target')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('X Variable')\n",
    "plt.ylabel('Y Variable')\n",
    "plt.title('Scatter Plot Colored by Target')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent from first principles\n",
    "\n",
    "Recall that a step of learning is given by the equation:\n",
    "\n",
    "\\begin{align}\\beta_\\text{next} &= \\beta - \\alpha \\nabla \\mathscr {L(\\mathbf\\beta)} \\\\\n",
    "\\text{where} &: \\nonumber\\\\\n",
    "\\beta &: \\text{the parameter vector} \\begin{pmatrix} \\beta_0\\\\ \\beta_1 \\end{pmatrix}\\nonumber\\\\\n",
    "\\beta_\\text{next} &: \\text{the next value of the parameter vector} \\nonumber\\\\\n",
    "\\alpha &: \\text{the learning rate} \\nonumber\\\\\n",
    "\\mathscr{L(\\mathbf\\beta)} &: \\text{the loss function}\\nonumber\\\\\n",
    "\\end{align}\n",
    "\n",
    "Let us consider the binary cross entropy function for a categorical target, in this case whether the target is 0 or 1, with only the `sepal length` as predictor.\n",
    "\n",
    "\\begin{align}\\mathscr{L} &= -\\sum_{i=1}^n (t_i \\log( \\widehat{t}_i) + (1-t_i)\\ \\log(1 - \\widehat{t}_i))\\\\\n",
    "&= - \\sum_{i=1}^n [t_i \\log(\\sigma(\\beta_0  + \\beta_1 x_i)) + (1-t_i)  \\log(1 - \\sigma(\\beta_0 + \\beta_1 x_i))]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The expression for the gradient of the loss is, therefore:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla \\mathscr {L}(\\mathbf\\beta) = \\begin{pmatrix} \\frac{\\partial L}{\\partial\\beta_0} \\\\ \\frac{\\partial L}{\\partial\\beta_1} \\end{pmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let us work out each of the component derivatives:\n",
    "\n",
    "\\begin{align}\\frac{\\partial \\mathscr L}{\\partial \\beta_0} &= - \\sum_{i=1}^n (t_i  (1 - \\widehat{t}_i) - (1-t_i)\\widehat{t}_i)\\\\\n",
    "\\frac{\\partial \\mathscr L}{\\partial \\beta_1} &= - \\sum_{i=1}^n (t_i  (1 - \\widehat{t}_i) - (1-t_i)\\widehat{t}_i)  x_i\\\\\n",
    "\\end{align}\n",
    "\n",
    "Therefore, the gradient descent step can be expressed as:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_{0, next} &= \\beta_0 + \\alpha\\sum_{i=1}^n (t_i  (1 - \\widehat{t}_i) - (1-t_i)\\widehat{t}_i)\\\\\n",
    "\\beta_{1, next} &= \\beta_1 + \\alpha\\sum_{i=1}^n (t_i  (1 - \\widehat{t}_i) - (1-t_i)\\widehat{t}_i)  x_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We are going to perform a batch gradient descent, i.e. use all of the training data to compute loss in each step. Let us start with a small learning rate, say $\\alpha = 10^{-5}$. We need a stopping criterion for our learning process.\n",
    "\n",
    "For simplicity, let us stop after say 200 epochs, i.e. running the gradient descent for a two hundred steps.\n",
    "\n",
    "**Definition: EPOCH**\n",
    "While training, an epoch is a complete cycle through the entire dataset. In other words, each datum in the dataset should have contributed to the learning in that cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:59.989125Z",
     "start_time": "2021-10-14T20:08:59.983961Z"
    }
   },
   "outputs": [],
   "source": [
    "α = pow(10, -3)\n",
    "β_0, β_1 =  4, 4 #np.random.normal(0,5,2) # initialize the parameters to some random values.\n",
    "epochs = 400\n",
    "X, T = restricted_df['x'].values, restricted_df['target'].values\n",
    "α, β_0, β_1, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid (x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.492565Z",
     "start_time": "2021-10-14T20:09:00.036838Z"
    }
   },
   "outputs": [],
   "source": [
    "intermediates = pd.DataFrame(columns=['epoch', 'β0', 'β1', 'loss'])\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    \n",
    "    # compute the gradients\n",
    "    dβ_0    = -sum([(ti *(1 - sigmoid(β_0 + β_1*xi)) - (1-ti) * (sigmoid(β_0 + β_1*xi))) for xi, ti in zip (X, T)]) \n",
    "    dβ_1    = -sum([xi*(ti *(1 - sigmoid(β_0 + β_1*xi)) - (1-ti) * (sigmoid(β_0 + β_1*xi))) for xi, ti in zip (X, T)])\n",
    "    \n",
    "    # gradient descent step\n",
    "    β_0     = (β_0 - α*dβ_0)\n",
    "    β_1     = (β_1 - α*dβ_1)\n",
    "    \n",
    "    # update the loss function\n",
    "    loss    = -sum([ (ti * np.log(sigmoid(β_0 +β_1*xi)) + (1-ti) * np.log(1-sigmoid(β_0 +β_1*xi)))for xi, ti in zip (X, T)])\n",
    "    \n",
    "    # store the values for later visualization\n",
    "    intermediates.loc[intermediates.shape[0]] = [epoch, β_0, β_1, loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us preview some of the rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.546327Z",
     "start_time": "2021-10-14T20:09:00.536863Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)     \n",
    "intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimization with epochs\n",
    "\n",
    "Let us now see how the learning of the parameters $\\beta_0$ and $\\beta_1$ happens with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.822406Z",
     "start_time": "2021-10-14T20:09:00.589645Z"
    },
    "cell_style": "split",
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(intermediates.epoch, intermediates.β0, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.title(r\"\"\"$\\beta_0\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:01.644692Z",
     "start_time": "2021-10-14T20:09:01.418618Z"
    },
    "cell_style": "split",
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(intermediates.epoch, intermediates.β1, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "plt.title(r\"\"\"$\\beta_1\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let us see how the loss (binary cross entropy loss) decreased with the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:03.344341Z",
     "start_time": "2021-10-14T20:09:03.044690Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(intermediates.epoch, intermediates.loss, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$L\\longrightarrow$');\n",
    "plt.title(r\"\"\"$L\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T16:24:02.087543Z",
     "start_time": "2021-10-12T16:24:02.084409Z"
    },
    "cell_style": "center"
   },
   "source": [
    "## Loss contour plots in the parameter space\n",
    "\n",
    "Let us start by creating a grid mesh of points in the hypothesis space, each point corresponding to a particular hypothesis's parameter value $\\beta = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\end{pmatrix}$. Next, let us compute the value of the loss function, $\\mathscr{L}$, as the sum-squared errors at each of these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:06.640670Z",
     "start_time": "2021-10-14T20:09:04.949450Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(β0:float, β1:float) -> float:\n",
    "     return -sum([ (ti * np.log(sigmoid(β0 +β1*xi)) + (1-ti) * np.log(1-sigmoid(β0 +β1*xi)))for xi, ti in zip (X, T)])\n",
    "\n",
    "# Create the mesh grid of values.\n",
    "b0     = np.linspace(-6, 6, 1000)\n",
    "b1     = np.linspace(-3,9,1000)\n",
    "B0, B1 = np.meshgrid(b0, b1)\n",
    "rss    = loss(B0, B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the contour surfaces (curves of equal loss or errors). Because we had standardized the data, the contour plot is elliptic will low eccentricity, i.e. almost circular; however, because we have drawn the plot in a 2:1 ratio, it looks more elliptical than it is. Change the plot size below to a square (20,20) for the figure size, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:07.276382Z",
     "start_time": "2021-10-14T20:09:06.684154Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "levels=[56, 65, 80, 100, 120, 140, 160, 180, 200]\n",
    "plt.figure(figsize=(20,10))\n",
    "contour_plot = plt.contour(B0, B1, rss, levels, colors='black', linestyles='dashed', linewidths=1, )\n",
    "plt.clabel(contour_plot, inline=1, fontsize=10)\n",
    "contour_plot = plt.contourf(B0, B1, rss,alpha=0.7)\n",
    "plt.xlabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "plt.title(r'\\textbf{Loss function contour plot}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss surface and the contour plots in the parameter plane\n",
    "\n",
    "Let us see the relationship between the loss surface, and its projection on the $\\beta$-hyperplane. \n",
    "\n",
    "**Note: in order to show the loss surface above the contour plot, we have artificially added 2000 to the residual loss, in order to lift the loss surface. To see the correct loss surface, reset the `loss_lift` to zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:08.825488Z",
     "start_time": "2021-10-14T20:09:07.455292Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(20,30))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "loss_lift = 100 # artificially lifing the loss surface to better show the contour plot below it.\n",
    "\n",
    "#Plot the loss surface\n",
    "ax.plot_surface(B0, B1, loss_lift + (rss),\n",
    "                cmap='viridis', alpha=0.9);\n",
    "\n",
    "levels=[56, 65, 80, 100, 120, 140, 160, 180, 200]\n",
    "\n",
    "# Plot the contours\n",
    "ax.contourf(B0, B1, rss, levels, offset = 0, alpha=0.5)\n",
    "plt.xlabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "ax.set_zlabel(r'$Loss\\longrightarrow$');\n",
    "\n",
    "# Plot the learning journey\n",
    "ax.plot(intermediates.β0, intermediates.β1, intermediates.loss + loss_lift,  color='red',zdir='z', linewidth=6)\n",
    "ax.scatter(intermediates.β0, intermediates.β1, s=20,  color='salmon',zdir='z')\n",
    "plt.title(r'\\textbf {The loss surface and its contour plot}');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
