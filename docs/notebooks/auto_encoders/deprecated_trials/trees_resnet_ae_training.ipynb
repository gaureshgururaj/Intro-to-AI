{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We walk through a vanilla AutoEncoder on the Trees dataset consisting of Oak and Weeping Willow trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from svlearn.config.configuration import ConfigurationMixin\n",
    "from svlearn.trees.preprocess import Preprocessor\n",
    "from svlearn.trees.tree_dataset import TreeDataset\n",
    "from svlearn.auto_encoders.auto_encoder_multi_channel_util import (train_autoencoder,\n",
    "                                                        visualize_reconstruction,\n",
    "                                                        get_latent_representations\n",
    ")\n",
    "from svlearn.auto_encoders.vanilla_resnet_auto_encoder import AutoencoderResnet\n",
    "from svlearn.auto_encoders.auto_encoder_util import (   sample_from_gmm,\n",
    "                                                        generate_images_from_latent,\n",
    "                                                        visualize_generated_images,\n",
    "                                                        visualize_interpolations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config and paths for the tree images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigurationMixin().load_config()\n",
    "data_dir = config['tree-classification']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the images as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df, val_df, label_encoder = preprocessor.preprocess(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take them through the same tensor transforms as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train_transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.RandomResizedCrop(224 , scale = (0.5, 1)), # Randomly crop and resize to 224x224\n",
    "    v2.RandomHorizontalFlip(p=0.5),       # Randomly flip the image horizontally with a 50% chance\n",
    "    v2.ColorJitter(brightness=0.4 , contrast=0.4, saturation=0.4), # randomly change the brightness , contrast and saturation of images\n",
    "    v2.ToDtype(torch.float32, scale=True), # ensure te tensor is of float datatype\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize tensor \n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.Resize(size=(224 , 224)),  # resize all images to a standard size suitable for the cnn model\n",
    "    v2.ToDtype(torch.float32, scale=True), # ensure te tensor is of float datatype\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize tensor \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TreeDataset(train_df, transform=train_transform)\n",
    "val_dataset = TreeDataset(val_df, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify device (either cuda or cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoencoderResnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training on the vanilla autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "train_autoencoder(model, train_loader, val_loader, num_epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the reconstruction (top row being original images and bottom being reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random shuffles of the val loader to visualize different samples each time.\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True) \n",
    "# Call this to monitor reconstruction\n",
    "visualize_reconstruction(model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate images from hidden vectors using the decoder of the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect latent representations from training data\n",
    "latent_data = get_latent_representations(model, train_loader, device=device)\n",
    "\n",
    "# Fit a Gaussian Mixture Model with 2 components\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(latent_data)\n",
    "\n",
    "# Sample from the GMM\n",
    "latent_samples = sample_from_gmm(gmm, num_samples=10)\n",
    "\n",
    "# Generate images from the latent samples\n",
    "generated_images = generate_images_from_latent(model, latent_samples, device=device)\n",
    "\n",
    "# Visualize the generated images\n",
    "visualize_generated_images(generated_images, is_color=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize interpolations between the reconstruction of two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create random shuffle of train dataset to pick random 2 images every time.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "first_train_batch, _ = next(iter(train_loader))\n",
    "# Visualize interpolations\n",
    "# Assuming image1 and image2 are samples from your dataset (PIL images already transformed to tensor)\n",
    "image1 = first_train_batch[0]  # First image\n",
    "image2 = first_train_batch[1]  # Second image\n",
    "\n",
    "# Visualize interpolation between the two images\n",
    "visualize_interpolations(model, image1, image2, num_steps=10, is_color=True, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
