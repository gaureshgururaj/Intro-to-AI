{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Call Classification\n",
    "\n",
    "The goal of the exercise is to classify audio samples of two different bird calls using CNN. But how can we classify audio using CNNs that are generally good with images?\n",
    "\n",
    "Raw audio is like hearing a conversation in an alien language—wavy lines that don’t make much sense to the eye or the ear. What if you could see the sound in a way that reveals its hidden patterns?\n",
    "\n",
    "That’s where mel-spectrograms come in. They’re like taking a picture of sound that reveals important features—It turns a chaotic mess of sound into a neat, colorful grid. This grid represents how energy is distributed across different frequencies over time. Mel-spectrograms take into account how we humans naturally hear sounds, making it much easier for a Convolutional Neural Network (CNN) to spot differences in things like speech, music, or even a bird tweeting.\n",
    "\n",
    "So, why do we transform audio into mel-spectrograms? Because CNNs are visual creatures—they thrive on images! By converting audio into something visual, like a spectrogram, we let our CNN use its superpowers to analyze and classify those sounds with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import joblib\n",
    "import IPython\n",
    "\n",
    "from svlearn.config.configuration import ConfigurationMixin\n",
    "from svlearn.train.simple_trainer import train_simple_network\n",
    "from svlearn.bird_call.birdcall_cnn import BirdCallCNN\n",
    "from svlearn.bird_call.spectrogram_dataset import SpectrogramDataset\n",
    "from svlearn.bird_call.preprocess import Preprocessor\n",
    "from svlearn.train.train_utils import split_dataset\n",
    "from svlearn.common.utils import ensure_directory\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigurationMixin().load_config()\n",
    "data_dir = config['bird-call-classification']['data']\n",
    "results_dir = config['bird-call-classification']['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Audio files\n",
    "\n",
    "- **Step 1: Loading and Resampling Audio Files**\n",
    "We begin by loading all the raw audio files from the specified directory. Since audio files can have different sample rates, we resample each one to a common sample rate to ensure consistency across the dataset.\n",
    "\n",
    "- **Step 2: Trimming or Padding Audio Files**\n",
    "Next, we standardize the duration of the audio files. Files longer than the target duration are trimmed, and those shorter are padded with silence to match the desired length. This ensures that all audio inputs have the same length, which is essential for uniform processing.\n",
    "\n",
    "- **Step 3: Converting to Mel-Spectrograms**\n",
    "Once the audio files are prepared, we convert each one into a mel-spectrogram. This transformation allows us to represent the audio in a format that a CNN can work with, as the network processes images, and mel-spectrograms provide a visual representation of sound.\n",
    "\n",
    "- **Step 4: Saving Mel-Spectrograms as Numpy Arrays**\n",
    "After converting the audio to mel-spectrograms, we save these spectrograms as numpy arrays in a separate processed directory. This step organizes the data into a format that is easy to load and use during model training.\n",
    "\n",
    "- **Step 5: Creating an Index File**\n",
    "Finally, we create an index file that maps each raw audio file to its corresponding processed spectrogram. This index file also includes the labels for each audio sample. It will be used by our custom dataset loader to associate each input with its label during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_preprocess = True # if running for the first time set this to True\n",
    "\n",
    "if to_preprocess:\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.fit_transform(data_dir)\n",
    "    joblib.dump(preprocessor, f\"{results_dir}/preprocessor.joblib\")\n",
    "\n",
    "else:\n",
    "    preprocessor = joblib.load(f\"{results_dir}/preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "\n",
    "### Create a custom dataset\n",
    "Use the processed mel-spectrograms and the index file to create a custom dataset. This dataset will load the spectrograms along with their associated labels. Once the dataset is ready, split it into two sets: one for training the model and another for evaluating its performance. This ensures that the model is trained on one subset of data and tested on unseen data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrogramDataset(data_dir)\n",
    "train_dataset , eval_dataset = split_dataset(dataset, eval_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Bird Calls\n",
    "\n",
    "#### Common Cuckoo\n",
    "\n",
    "Let's take samples from our dataset and listen to the cuckoo and the sparrow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "audio_path = dataset.get_audio_path(idx)\n",
    "IPython.display.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's view what our CNN model would see, i.e. let's plot the mel-spectrogram. On comparing the spectrograms do we see any visual difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_spectrogram(idx , title=\"Mel-Spectrogram (Common Cuckoo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Song Sparrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 604\n",
    "\n",
    "audio_path = dataset.get_audio_path(idx)\n",
    "\n",
    "IPython.display.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_spectrogram(idx, title=\"Mel-Spectrogram (Song Sparrow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the frequency of the sparrow is much higher than the cuckoo. Also we can see the distinct and strong pattern that the cuckoo call creates in the spectrogram.\n",
    "\n",
    "### Instantiate the CNN Model\n",
    "Let's set up a Convolutional Neural Network (CNN) designed to classify the audio samples into two target classes. This involves defining the network architecture, specifying the input size to match the shape of the mel-spectrograms, and setting the output layer to have two neurons, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BirdCallCNN(2)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = SpectrogramDataset(data_dir)\n",
    "train_dataset , eval_dataset = split_dataset(dataset, eval_split=0.2)\n",
    "\n",
    "print(train_dataset[0][0].shape)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=eval_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the same train simple network class from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory(results_dir)\n",
    "result = train_simple_network(\n",
    "                        model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_func=nn.CrossEntropyLoss(),\n",
    "                        train_loader=train_loader,\n",
    "                        test_loader=val_loader,\n",
    "                        epochs=5,\n",
    "                        score_funcs={'accuracy': accuracy_score},\n",
    "                        classify=True,\n",
    "                        checkpoint_file=f\"{results_dir}/cnn-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Trained Model\n",
    "\n",
    "Let's test our model with some audio files that the model has not yet seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import os\n",
    "\n",
    "test_dir = f\"{data_dir}/test\"\n",
    "audio_files = os.listdir(test_dir)\n",
    "\n",
    "\n",
    "file_path = os.path.join(test_dir , audio_files[0])\n",
    "\n",
    "IPython.display.Audio(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for inference\n",
    "We use the preprocessor that we fitted while training for trasforming the audio file to spectrogram and getting the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram , label = preprocessor.transform_audio(file_path , 'common_cuckoo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's load the best model we have from the checkpoint files we saved while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{results_dir}/cnn-model.pt\", 'rb') as f:\n",
    "    checkpoint = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model's parameters from the saved checkpoint's model_state_dict which stores the learned parameters. We set the model to evaluation mode to skip batch normalizations and dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the spectrogram to the model and see it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_hat = model(torch.tensor(spectrogram).unsqueeze(0))\n",
    "\n",
    "prediction = preprocessor.labels_to_names[int(y_hat.argmax())]\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds good! Try the same for Song Sparrow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Pretrained Models\n",
    "\n",
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load the pretrained ResNet18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the first convolution layer to accept a single-channel (grayscale) input\n",
    "# The original conv1 has an input size of (3, 224, 224), we change it to (1, 128, 431)\n",
    "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Optionally, you can replace the final fully connected (FC) layer\n",
    "# Here, we assume you have 10 birdcall classes to predict, modify it accordingly\n",
    "num_classes = 2\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_simple_network(\n",
    "                        model=resnet18,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_func=nn.CrossEntropyLoss(),\n",
    "                        train_loader=train_loader,\n",
    "                        test_loader=val_loader,\n",
    "                        epochs=10,\n",
    "                        score_funcs={'accuracy': accuracy_score},\n",
    "                        classify=True,\n",
    "                        checkpoint_file=f\"{results_dir}/resnet-model-01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our simpler BirdCallCNN seems to perform much better than finetuning in this case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
