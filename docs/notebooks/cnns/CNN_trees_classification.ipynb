{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise: Classifying Willow Tree vs. Oak Tree Using CNN\n",
    "\n",
    "## Objective:\n",
    "The goal of this lab is to design and implement a Convolutional Neural Network (CNN) to classify between two types of trees: **Willow** and **Oak**. You will work with a dataset that contains images of these two tree classes, divided into subfolders. The exercise will guide you through creating a custom dataset class, designing a CNN model, and writing a training script to perform the binary classification.\n",
    "\n",
    "\n",
    "## Tasks Breakdown:\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "   Create a PyTorch Dataset class to load and preprocess images from the `trees` folder.\n",
    "     - **Folder Structure:**\n",
    "       - `trees/Willowtree/` – contains images of Willow trees.\n",
    "       - `trees/Oaktree/` – contains images of Oak trees.\n",
    "   - **Objective:** \n",
    "     - Write a custom PyTorch Dataset class to:\n",
    "       - Load images from both folders.\n",
    "       - Convert images to PyTorch tensors.\n",
    "       - Apply standard image transformations (e.g., resizing, normalization).\n",
    "       - Assign labels: 0 for Willow trees and 1 for Oak trees.\n",
    "\n",
    "\n",
    "### 2. CNN Architecture Design\n",
    "  Design a CNN architecture for binary classification.\n",
    "   - **Objective:**\n",
    "     - Create a PyTorch CNN model that:\n",
    "       - Contains several convolutional, activation, and pooling layers.\n",
    "       - Includes fully connected layers at the end for binary classification.\n",
    "\n",
    "\n",
    "### 3. Training Script\n",
    "  Implement a training script to train the CNN model on the dataset.\n",
    "   - **Objective:**\n",
    "     - Write a script to:\n",
    "       - Split the dataset into training and validation sets.\n",
    "       - Define a loss function (binary cross-entropy) and an optimizer (e.g., Adam).\n",
    "       - Train the model for a specified number of epochs.\n",
    "       - Evaluate the model on the validation set after each epoch.\n",
    "       - Output training and validation accuracy at each step.\n",
    "   \n",
    "\n",
    "### 4. Evaluation\n",
    "  Evaluate the trained model and report performance metrics.\n",
    "   - **Objective:**\n",
    "     - After training, evaluate the model on a test dataset and report:\n",
    "       - Accuracy.\n",
    "       - Precision and recall.\n",
    "       - Confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# svlearn\n",
    "from svlearn.trees.tree_dataset import TreeDataset\n",
    "from svlearn.trees.preprocess import Preprocessor\n",
    "from svlearn.config.configuration import ConfigurationMixin\n",
    "from svlearn.train.visualization_utils import (\n",
    "    show_image_with_denormalization,\n",
    "    show_sample_image,\n",
    "    visualize_classification_training_results,\n",
    ")\n",
    "from svlearn.train.simple_trainer import train_simple_network\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigurationMixin().load_config()\n",
    "data_dir = config['tree-classification']['data']\n",
    "results_dir = config['tree-classification']['results']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "Let's load the images from our `data_dir`. The `Preprocessor` does all the preprocessing including loading image paths , label encoding, and spliting the dataset for training and evaluation. \n",
    "\n",
    "The preprocess method returns \n",
    " - `train_df` , `test_df` - each containing image paths and their corresponding integer labels \n",
    " - `label_encoder` which we will later use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df, val_df, label_encoder = preprocessor.preprocess(data_dir)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Dataset\n",
    "\n",
    "### Image Transformations\n",
    "\n",
    "Let's load the images into our tree dataset and apply some transformations. while transforming images for training we want to create as much variability as possible so that the model can generalize well. We randomly distort the images to make it difficult for the model to overfit. But while evaluating we don't apply these random transformations and try to retain the original image as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "# \n",
    "train_transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.RandomResizedCrop(224 , scale = (0.5, 1)), # Randomly crop and resize to 224x224\n",
    "    v2.RandomHorizontalFlip(p=0.5),       # Randomly flip the image horizontally with a 50% chance\n",
    "    v2.ColorJitter(brightness=0.4 , contrast=0.4, saturation=0.4), # randomly change the brightness , contrast and saturation of images\n",
    "    v2.ToDtype(torch.float32, scale=True), # ensure te tensor is of float datatype\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize tensor \n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.Resize(size=(224 , 224)),  # resize all images to a standard size suitable for the cnn model\n",
    "    v2.ToDtype(torch.float32, scale=True), # ensure te tensor is of float datatype\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize tensor \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization** is an essential preprocessing step in computer vision tasks. It rescales pixel values in an image to a common range, typically between 0 and 1, or -1 and 1. \n",
    "\n",
    "Why Normalize?\n",
    " - **Consistency**: Images can have different lighting, contrast, or color variations. Normalization reduces the impact of these differences, making images more consistent for the model to learn from.\n",
    "\n",
    " - **Prevents Large Gradients**: If the input values are too large, it can cause large gradient values during backpropagation, leading to instability or slow learning. Normalization keeps the gradients in a reasonable range, making the learning process smoother.\n",
    "\n",
    "\n",
    "In this case, the mean and std (standard deviation) are specific values chosen for typical images from the ImageNet dataset (which we can use for general image datasets too). These values ensure that the pixel values have a mean of 0 and a standard deviation of 1, which helps the CNN learn more effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TreeDataset(train_df, transform=train_transform)\n",
    "val_dataset = TreeDataset(val_df, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the inputs look?\n",
    "\n",
    "Now that the dataset is created, let's take a sample from the test dataset of an image with an Oak tree. After all these transformations what does the image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_image(val_dataset , 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel value are not too pleasing to the human eye. This is specifically because of `Normalization`. We can negate this transformation (but retain all other previous transformations )by denormalizing. i.e. we multiply the standard deviation and add the mean back to the pixel value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_denormalization(val_dataset, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's also see a sample of a willow tree image, this time from the train dataset. Notice that each time you run the cell below the image is slightly different. This is because of the `train_transform` we applied previously. Every time the dataloader requests a sample, the dataset thus returns a slightly modified version of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_denormalization(train_dataset, 205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "We create dataloaders from the train and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN Model\n",
    "Next Let's design a simple CNN architecture that uses the tools we learned in theory class. We want the model (with all it's weights and biases) classify the two types of trees: Weeping Willow and Oak Tree. \n",
    "\n",
    "Each block will have a convolution layer,  activation , a batch norm layer and a pooling. \n",
    " - **Convolution Layer** applies filters (small matrices) to input images to detect specific patterns. As the filter slides across the image, it creates feature maps, highlighting the presence of these patterns. Each filter learns to detect different patterns, and deeper layers learn more complex features.\n",
    " - **Activation Layer** introduces non-linearity to the network.\n",
    " - **Batch Normalization Layer** standardizes the input to a layer by scaling and shifting it, ensuring that the mean is close to 0 and the variance is near 1.\n",
    " - **Max Pooling** reduce the spatial dimensions (height and width) of feature maps while preserving the most important information. It makes the network more **robust to minor distortions** or varations in the input\n",
    "\n",
    "After colvolutions , we enter the familiar territory of fully connected layers which in the end produce outputs representing the model's prediction for both the classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Convolution Block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=0),     # ( B , 3 , 224 , 224 ) ->  ( B , 6 , 220 , 220 )\n",
    "            nn.BatchNorm2d(num_features=6),                                     \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                  # ( B , 6 , 220 , 220 ) ->  ( B , 6 , 110 , 110 )\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        # Convolution Block 2\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0),    # ( B , 6 , 110 , 110 ) ->  ( B , 16 , 106 , 106 )\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                   # ( B , 16 , 106 , 106 ) ->  ( B , 16 , 53 , 53 )\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        # Convolution Block 3\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4),              # ( B , 16 , 53 , 53 ) ->  ( B , 32 , 50 , 50 )                           \n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                   # ( B , 32 , 50 , 50 )   ->  ( B , 32 , 25 , 25 ) \n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "            nn.Flatten(), # Change from 2D image to 1D tensor to be able to pass inputs to linear layer\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Linear Block 1\n",
    "            nn.Linear(in_features=32 * 25 * 25, out_features=180),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        # Linear block 2\n",
    "            nn.Linear(in_features=180, out_features=84),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "            nn.Linear(in_features=84, out_features=num_classes)\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define our optimizer to update the model parameters and run our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 0.001)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Script\n",
    "\n",
    "We will reuse a classification trainer script we previously used to do binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_simple_network(\n",
    "                        model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        lr_scheduler=scheduler,\n",
    "                        loss_func=nn.CrossEntropyLoss(),\n",
    "                        train_loader=train_loader,\n",
    "                        test_loader=val_loader,\n",
    "                        epochs=10,\n",
    "                        score_funcs={'accuracy': accuracy_score},\n",
    "                        classify=True,\n",
    "                        checkpoint_file=f\"{results_dir}/cnn-model-trial.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the results to see the learning progress of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_classification_training_results(result['train loss'] , \n",
    "                                          result['test loss'] , \n",
    "                                          result['train accuracy'] , \n",
    "                                          result['test accuracy'], \n",
    "                                          dir_path=results_dir, \n",
    "                                          filename=\"image classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "Next let's test out our model with an image. We load the saved model weights and biases from our checkpoint directory and reset our model's paramters to these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"{results_dir}/cnn-model-trial.pt\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image from file\n",
    "Download an image from the internet and paste and assign it's path to `img_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = \"/home/chandar/data/trees/Oak/images379.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Convert the tensor image back to PIL image for display\n",
    "image = v2.ToPILImage()(image)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform to the image to convert to the image into the input that our model expects. By doing `unsqueeze.()` we add an additional dimension that represents a batch (of size 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = test_transform(image).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction\n",
    "Did the model classify the image correctly? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(input)\n",
    "prediction = torch.argmax(y_hat)\n",
    "label_encoder.inverse_transform([prediction])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning a Pretrained model\n",
    "\n",
    "### VGG16\n",
    "\n",
    "We designed our CNN model from scratch, let us know use a Pretrained model - VGG and finetune the last few weights of the network with the help of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the VGG16 model\n",
    "vgg_model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the feature extraction layers\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier for 2 classes\n",
    "vgg_model.classifier[6] = nn.Linear(4096, 2) \n",
    "\n",
    "optimizer = torch.optim.Adam(vgg_model.classifier[6].parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_simple_network(\n",
    "                        model=vgg_model,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_func=nn.CrossEntropyLoss(),\n",
    "                        train_loader=train_loader,\n",
    "                        test_loader=val_loader,\n",
    "                        epochs=10,\n",
    "                        score_funcs={'accuracy': accuracy_score},\n",
    "                        classify=True,\n",
    "                        checkpoint_file=f\"{results_dir}/vgg-model-01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classification_training_results(result['train loss'] , \n",
    "                                          result['test loss'] , \n",
    "                                          result['train accuracy'] , \n",
    "                                          result['test accuracy'], \n",
    "                                          dir_path=results_dir, \n",
    "                                          filename=\"image classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
